{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate q-learning\n",
    "\n",
    "In this notebook you will teach a lasagne neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. For example, it translates to TensorFlow almost line-to-line. However, we recommend you to stick to theano/lasagne unless you're certain about your skills in the framework of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS='floatX=float32'\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-26 01:56:59,590] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1188e6c88>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEkhJREFUeJzt3XGs3eV93/H3ZzaBLMlqCHeWZ5uZtt4iOjWG3RFQoomC\n0gKLZip1EWxqUIR0qUSkRI3aQie1iTSkVlrDFm1DdQuNU6UhlCTDQqwpdZCq/BHIJXEcG4fmJnFk\nWwbfJECSRaMz+e6P+5icXa59z73nHl/fh/dLOjq/3/N7fr/zfeDoc8997u/xSVUhSerPP1jtAiRJ\n42HAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1amwBn+T6JM8kmUly57heR5K0sIzjPvgk64C/A94JHAW+\nBNxSVU+v+ItJkhY0rk/wVwIzVfWtqvp74AFg55heS5K0gPVjuu5m4MjA/lHgbafrfPHFF9e2bdvG\nVIokrT2HDx/mu9/9bka5xrgCflFJpoApgEsuuYTp6enVKkWSzjmTk5MjX2NcUzTHgK0D+1ta2yuq\naldVTVbV5MTExJjKkKTXrnEF/JeA7UkuTfI64GZgz5heS5K0gLFM0VTVySTvAz4HrAPur6qD43gt\nSdLCxjYHX1WPAo+O6/qSpDNzJaskdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpUwa8\nJHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE6N9JV9SQ4DPwReBk5W\n1WSSi4BPAduAw8C7q+r50cqUJC3VSnyC/6Wq2lFVk23/TmBvVW0H9rZ9SdJZNo4pmp3A7ra9G7hp\nDK8hSVrEqAFfwF8neSrJVGvbWFXH2/azwMYRX0OStAwjzcED76iqY0n+MfBYkq8PHqyqSlILndh+\nIEwBXHLJJSOWIUmab6RP8FV1rD2fAD4LXAk8l2QTQHs+cZpzd1XVZFVNTkxMjFKGJGkByw74JG9I\n8qZT28AvAweAPcCtrdutwMOjFilJWrpRpmg2Ap9Ncuo6f1FVf5XkS8CDSW4DvgO8e/QyJUlLteyA\nr6pvAW9doP17wHWjFCVJGp0rWSWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAl\nqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROLRrwSe5PciLJ\ngYG2i5I8luQb7fnC1p4kH00yk2R/kivGWbwk6fSG+QT/MeD6eW13Anurajuwt+0D3ABsb48p4N6V\nKVOStFSLBnxV/S3w/XnNO4HdbXs3cNNA+8drzheBDUk2rVSxkqThLXcOfmNVHW/bzwIb2/Zm4MhA\nv6Ot7VWSTCWZTjI9Ozu7zDIkSacz8h9Zq6qAWsZ5u6pqsqomJyYmRi1DkjTPcgP+uVNTL+35RGs/\nBmwd6LeltUmSzrLlBvwe4Na2fSvw8ED7e9rdNFcBLw5M5UiSzqL1i3VI8kngGuDiJEeB3wf+AHgw\nyW3Ad4B3t+6PAjcCM8CPgfeOoWZJ0hAWDfiquuU0h65boG8Bd4xalCRpdK5klaROGfCS1CkDXpI6\nZcBLUqcMeEnqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMG\nvCR1yoCXpE4Z8JLUqUUDPsn9SU4kOTDQ9qEkx5Lsa48bB47dlWQmyTNJfmVchUuSzmyYT/AfA65f\noP2eqtrRHo8CJLkMuBn4hXbO/0iybqWKlSQNb9GAr6q/Bb4/5PV2Ag9U1UtV9W1gBrhyhPokScs0\nyhz8+5Lsb1M4F7a2zcCRgT5HW9urJJlKMp1kenZ2doQyJEkLWW7A3wv8HLADOA780VIvUFW7qmqy\nqiYnJiaWWYYk6XSWFfBV9VxVvVxVPwH+hJ9OwxwDtg503dLaJEln2bICPsmmgd1fBU7dYbMHuDnJ\n+UkuBbYDT45WoiRpOdYv1iHJJ4FrgIuTHAV+H7gmyQ6ggMPA7QBVdTDJg8DTwEngjqp6eTylS5LO\nZNGAr6pbFmi+7wz97wbuHqUoSdLoXMkqSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOrXobZLSa9FT\nu25fsP1fTv3xWa5EWj4/wUtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcM\neEnqlAEvSZ1aNOCTbE3yeJKnkxxM8v7WflGSx5J8oz1f2NqT5KNJZpLsT3LFuAchSXq1YT7BnwQ+\nWFWXAVcBdyS5DLgT2FtV24G9bR/gBmB7e0wB96541ZKkRS0a8FV1vKq+3LZ/CBwCNgM7gd2t227g\npra9E/h4zfkisCHJphWvXJJ0Rkuag0+yDbgceALYWFXH26FngY1tezNwZOC0o61t/rWmkkwnmZ6d\nnV1i2ZKkxQwd8EneCHwa+EBV/WDwWFUVUEt54araVVWTVTU5MTGxlFOlVeG/Ba+1ZqiAT3Iec+H+\niar6TGt+7tTUS3s+0dqPAVsHTt/S2iRJZ9Ewd9EEuA84VFUfGTi0B7i1bd8KPDzQ/p52N81VwIsD\nUzmSpLNkmK/sezvw68DXkuxrbb8L/AHwYJLbgO8A727HHgVuBGaAHwPvXdGKJUlDWTTgq+oLQE5z\n+LoF+hdwx4h1SZJG5EpWSeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEv\nSZ0y4CWpUwa8JHXKgJfmeWrX7atdgrQiDHhpCH6bk9YiA16SOmXAS1KnDHhJ6pQBL0mdGuZLt7cm\neTzJ00kOJnl/a/9QkmNJ9rXHjQPn3JVkJskzSX5lnAOQJC1smC/dPgl8sKq+nORNwFNJHmvH7qmq\n/zzYOcllwM3ALwD/BPibJP+sql5eycIlSWe26Cf4qjpeVV9u2z8EDgGbz3DKTuCBqnqpqr4NzABX\nrkSxkqThLWkOPsk24HLgidb0viT7k9yf5MLWthk4MnDaUc78A0GSNAZDB3ySNwKfBj5QVT8A7gV+\nDtgBHAf+aCkvnGQqyXSS6dnZ2aWcKkkawlABn+Q85sL9E1X1GYCqeq6qXq6qnwB/wk+nYY4BWwdO\n39La/j9VtauqJqtqcmJiYpQxSJIWMMxdNAHuAw5V1UcG2jcNdPtV4EDb3gPcnOT8JJcC24EnV65k\nSdIwhrmL5u3ArwNfS7Kvtf0ucEuSHUABh4HbAarqYJIHgaeZuwPnDu+gkaSzb9GAr6ovAFng0KNn\nOOdu4O4R6pIkjciVrJLUKQNekjplwEsD/LIP9cSAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y\n4CWpUwa8JHXKgJekThnwktQpA16vCUmGeox6/pmuIZ1tBrwkdWqYL/yQXnMeOT71yva7Nu1axUqk\n5TPgpQGDwS6tdU7RSIsw9LVWDfOl2xckeTLJV5McTPLh1n5pkieSzCT5VJLXtfbz2/5MO75tvEOQ\nxsspGq1Vw3yCfwm4tqreCuwArk9yFfCHwD1V9fPA88Btrf9twPOt/Z7WT1oT3rVp16sCffJ2A15r\n0zBful3Aj9ruee1RwLXAv2/tu4EPAfcCO9s2wEPAf0uSdh3pnPbTMDfUtfYNNQefZF2SfcAJ4DHg\nm8ALVXWydTkKbG7bm4EjAO34i8CbV7JoSdLihgr4qnq5qnYAW4ArgbeM+sJJppJMJ5menZ0d9XKS\npHmWdBdNVb0APA5cDWxIcmqKZwtwrG0fA7YCtOM/A3xvgWvtqqrJqpqcmJhYZvmSpNMZ5i6aiSQb\n2vbrgXcCh5gL+l9r3W4FHm7be9o+7fjnnX+XpLNvmIVOm4DdSdYx9wPhwap6JMnTwANJ/hPwFeC+\n1v8+4M+TzADfB24eQ92SpEUMcxfNfuDyBdq/xdx8/Pz2/wP8uxWpTpK0bK5klaROGfCS1CkDXpI6\n5b8mqdcEb+TSa5Gf4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1\nyoCXpE4Z8JLUKQNekjplwEtSp4b50u0LkjyZ5KtJDib5cGv/WJJvJ9nXHjtae5J8NMlMkv1Jrhj3\nICRJrzbMvwf/EnBtVf0oyXnAF5L8r3bst6rqoXn9bwC2t8fbgHvbsyTpLFr0E3zN+VHbPa89zvTt\nCTuBj7fzvghsSLJp9FIlSUsx1Bx8knVJ9gEngMeq6ol26O42DXNPkvNb22bgyMDpR1ubJOksGirg\nq+rlqtoBbAGuTPIvgLuAtwD/CrgI+J2lvHCSqSTTSaZnZ2eXWLYkaTFLuoumql4AHgeur6rjbRrm\nJeDPgCtbt2PA1oHTtrS2+dfaVVWTVTU5MTGxvOolSac1zF00E0k2tO3XA+8Evn5qXj1JgJuAA+2U\nPcB72t00VwEvVtXxsVQvSTqtYe6i2QTsTrKOuR8ID1bVI0k+n2QCCLAP+I3W/1HgRmAG+DHw3pUv\nW5K0mEUDvqr2A5cv0H7tafoXcMfopUmSRuFKVknqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16S\nOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalT\nQwd8knVJvpLkkbZ/aZInkswk+VSS17X289v+TDu+bTylS5LOZCmf4N8PHBrY/0Pgnqr6eeB54LbW\nfhvwfGu/p/WTJJ1lQwV8ki3AvwH+tO0HuBZ4qHXZDdzUtne2fdrx61p/SdJZtH7Ifv8F+G3gTW3/\nzcALVXWy7R8FNrftzcARgKo6meTF1v+7gxdMMgVMtd2XkhxY1gjOfRczb+yd6HVc0O/YHNfa8k+T\nTFXVruVeYNGAT/Iu4ERVPZXkmuW+0Hyt6F3tNaaranKlrn0u6XVsvY4L+h2b41p7kkzTcnI5hvkE\n/3bg3ya5EbgA+EfAfwU2JFnfPsVvAY61/seArcDRJOuBnwG+t9wCJUnLs+gcfFXdVVVbqmobcDPw\n+ar6D8DjwK+1brcCD7ftPW2fdvzzVVUrWrUkaVGj3Af/O8BvJplhbo79vtZ+H/Dm1v6bwJ1DXGvZ\nv4KsAb2OrddxQb9jc1xrz0hjix+uJalPrmSVpE6tesAnuT7JM23l6zDTOeeUJPcnOTF4m2eSi5I8\nluQb7fnC1p4kH21j3Z/kitWr/MySbE3yeJKnkxxM8v7WvqbHluSCJE8m+Wob14dbexcrs3tdcZ7k\ncJKvJdnX7ixZ8+9FgCQbkjyU5OtJDiW5eiXHtaoBn2Qd8N+BG4DLgFuSXLaaNS3Dx4Dr57XdCeyt\nqu3AXn76d4gbgO3tMQXce5ZqXI6TwAer6jLgKuCO9v9mrY/tJeDaqnorsAO4PslV9LMyu+cV579U\nVTsGbolc6+9FmLsj8a+q6i3AW5n7f7dy46qqVXsAVwOfG9i/C7hrNWta5ji2AQcG9p8BNrXtTcAz\nbfuPgVsW6neuP5i7S+qdPY0N+IfAl4G3MbdQZn1rf+V9CXwOuLptr2/9stq1n2Y8W1ogXAs8AqSH\ncbUaDwMXz2tb0+9F5m4h//b8/+4rOa7VnqJ5ZdVrM7gidi3bWFXH2/azwMa2vSbH2359vxx4gg7G\n1qYx9gEngMeAbzLkymzg1Mrsc9GpFec/aftDrzjn3B4XQAF/neSptgoe1v578VJgFvizNq32p0ne\nwAqOa7UDvns196N2zd6qlOSNwKeBD1TVDwaPrdWxVdXLVbWDuU+8VwJvWeWSRpaBFeerXcuYvKOq\nrmBumuKOJP968OAafS+uB64A7q2qy4H/zbzbykcd12oH/KlVr6cMrohdy55LsgmgPZ9o7WtqvEnO\nYy7cP1FVn2nNXYwNoKpeYG7B3tW0ldnt0EIrsznHV2afWnF+GHiAuWmaV1actz5rcVwAVNWx9nwC\n+CxzP5jX+nvxKHC0qp5o+w8xF/grNq7VDvgvAdvbX/pfx9xK2T2rXNNKGFzNO3+V73vaX8OvAl4c\n+FXsnJIkzC1aO1RVHxk4tKbHlmQiyYa2/Xrm/q5wiDW+Mrs6XnGe5A1J3nRqG/hl4ABr/L1YVc8C\nR5L889Z0HfA0Kzmuc+APDTcCf8fcPOh/XO16llH/J4HjwP9l7ifybczNZe4FvgH8DXBR6xvm7hr6\nJvA1YHK16z/DuN7B3K+G+4F97XHjWh8b8IvAV9q4DgC/19p/FngSmAH+Eji/tV/Q9mfa8Z9d7TEM\nMcZrgEd6GVcbw1fb4+CpnFjr78VW6w5gur0f/ydw4UqOy5WsktSp1Z6ikSSNiQEvSZ0y4CWpUwa8\nJHXKgJekThnwktQpA16SOmXAS1Kn/h8EC377+O/tdgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119706240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) Q-learning: building the network\n",
    "\n",
    "In this section we will build and train naive Q-learning with theano/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is initializing input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "#create input variables. We'll support multiple states at once\n",
    "\n",
    "\n",
    "current_states = T.matrix(\"states[batch,units]\")\n",
    "actions = T.ivector(\"action_ids[batch]\")\n",
    "rewards = T.vector(\"rewards[batch]\")\n",
    "next_states = T.matrix(\"next states[batch,units]\")\n",
    "is_end = T.ivector(\"vector[batch] where 1 means that session just ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne import nonlinearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "#input layer\n",
    "l_states = InputLayer((None,)+state_dim)\n",
    "\n",
    "nnet = DenseLayer(l_states, 100, nonlinearity=nonlinearities.leaky_rectify)\n",
    "\n",
    "\n",
    "\n",
    "#output layer\n",
    "l_qvalues = DenseLayer(nnet,num_units=n_actions,nonlinearity=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Q-values for `current_states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get q-values for ALL actions in current_states\n",
    "predicted_qvalues = get_output(l_qvalues,{l_states:current_states})\n",
    "\n",
    "#predict q-values for next states\n",
    "predicted_next_qvalues = get_output(l_qvalues,{l_states:next_states})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compiling agent's \"GetQValues\" function\n",
    "get_qvalues = theano.function([current_states], predicted_qvalues, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = predicted_qvalues[T.arange(actions.shape[0]),actions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and `update`\n",
    "Here we write a function similar to `agent.update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Computing target q-values under \n",
    "gamma = 0.99\n",
    "target_qvalues_for_actions = rewards + gamma * predicted_next_qvalues.max(axis=1)\n",
    "\n",
    "#zero-out q-values at the end\n",
    "target_qvalues_for_actions = (1-is_end)*target_qvalues_for_actions\n",
    "\n",
    "#don't compute gradient over target q-values (consider constant)\n",
    "target_qvalues_for_actions = theano.gradient.disconnected_grad(target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mean squared error loss function\n",
    "loss = ((target_qvalues_for_actions - predicted_qvalues_for_actions)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all network weights\n",
    "all_weights = get_all_params(l_qvalues,trainable=True)\n",
    "\n",
    "#network updates. Note the small learning rate (for stability)\n",
    "updates = lasagne.updates.sgd(loss,all_weights,learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training function that resembles agent.update(state,action,reward,next_state) \n",
    "#with 1 more argument meaning is_end\n",
    "train_step = theano.function([current_states,actions,rewards,next_states,is_end],\n",
    "                             updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 0.25 #initial epsilon\n",
    "\n",
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    \n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #get action q-values from the network\n",
    "        q_values = get_qvalues([s])[0] \n",
    "        \n",
    "        if np.random.random() < epsilon:\n",
    "            a = np.random.randint(0, n_actions) \n",
    "        else:\n",
    "            a = q_values.argmax()\n",
    "            \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #train agent one step. Note that we use one-element arrays instead of scalars \n",
    "        #because that's what function accepts.\n",
    "        train_step([s],[a],[r],[new_s],[done])\n",
    "        \n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:150.630\tepsilon:0.00950\n",
      "mean reward:151.530\tepsilon:0.00903\n",
      "mean reward:155.210\tepsilon:0.00857\n",
      "mean reward:164.290\tepsilon:0.00815\n",
      "mean reward:176.850\tepsilon:0.00774\n",
      "mean reward:187.180\tepsilon:0.00735\n",
      "mean reward:203.650\tepsilon:0.00698\n",
      "mean reward:217.540\tepsilon:0.00663\n",
      "mean reward:233.260\tepsilon:0.00630\n",
      "mean reward:253.630\tepsilon:0.00599\n",
      "mean reward:291.660\tepsilon:0.00569\n",
      "mean reward:337.860\tepsilon:0.00540\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.01\n",
    "for i in range(100):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    epsilon*=0.95\n",
    "    \n",
    "    print (\"mean reward:%.3f\\tepsilon:%.5f\"%(np.mean(rewards),epsilon))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n",
    "    assert epsilon!=0, \"Please explore environment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon=0 #Don't forget to reset epsilon back to initial value if you want to go on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-26 02:00:40,940] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env=gym.make(\"CartPole-v0\");env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-26 02:00:42,776] Clearing 7 monitor files from previous run (because force=True was provided)\n",
      "[2017-10-26 02:00:42,780] Starting new video recorder writing to /Users/sergmiller/Documents/code/python/ml-mipt2/ml-mipt-part2/2017/seminars/7_approximate_rl/videos/openaigym.video.5.2640.video000000.mp4\n",
      "[2017-10-26 02:00:45,384] Starting new video recorder writing to /Users/sergmiller/Documents/code/python/ml-mipt2/ml-mipt-part2/2017/seminars/7_approximate_rl/videos/openaigym.video.5.2640.video000001.mp4\n",
      "[2017-10-26 02:00:47,700] Starting new video recorder writing to /Users/sergmiller/Documents/code/python/ml-mipt2/ml-mipt-part2/2017/seminars/7_approximate_rl/videos/openaigym.video.5.2640.video000008.mp4\n",
      "[2017-10-26 02:00:50,559] Starting new video recorder writing to /Users/sergmiller/Documents/code/python/ml-mipt2/ml-mipt-part2/2017/seminars/7_approximate_rl/videos/openaigym.video.5.2640.video000027.mp4\n",
      "[2017-10-26 02:00:56,916] Starting new video recorder writing to /Users/sergmiller/Documents/code/python/ml-mipt2/ml-mipt-part2/2017/seminars/7_approximate_rl/videos/openaigym.video.5.2640.video000064.mp4\n",
      "[2017-10-26 02:01:00,377] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/sergmiller/Documents/code/python/ml-mipt2/ml-mipt-part2/2017/seminars/7_approximate_rl/videos')\n"
     ]
    }
   ],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#unwrap \n",
    "env = env.env.env\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later\n",
    "\n",
    "#Warning! If you keep seeing error that reads something like\"DoubleWrapError\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.5.2640.video000001.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
