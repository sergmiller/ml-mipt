{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate q-learning\n",
    "\n",
    "In this notebook you will teach a lasagne neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. For example, it translates to TensorFlow almost line-to-line. However, we recommend you to stick to theano/lasagne unless you're certain about your skills in the framework of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS='floatX=float32'\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12c097b00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121bd3ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) Q-learning: building the network\n",
    "\n",
    "In this section we will build and train naive Q-learning with theano/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is initializing input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "#create input variables. We'll support multiple states at once\n",
    "\n",
    "\n",
    "current_states = T.matrix(\"states[batch,units]\")\n",
    "actions = T.ivector(\"action_ids[batch]\")\n",
    "rewards = T.vector(\"rewards[batch]\")\n",
    "next_states = T.matrix(\"next states[batch,units]\")\n",
    "is_end = T.ivector(\"vector[batch] where 1 means that session just ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergmiller/anaconda3/envs/py342/lib/python3.4/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from lasagne import nonlinearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "#input layer\n",
    "l_states = InputLayer((None,)+state_dim)\n",
    "\n",
    "nnet = DenseLayer(l_states, 100, nonlinearity=nonlinearities.leaky_rectify)\n",
    "\n",
    "#output layer\n",
    "l_qvalues = DenseLayer(nnet,num_units=n_actions,nonlinearity=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Q-values for `current_states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get q-values for ALL actions in current_states\n",
    "predicted_qvalues = get_output(l_qvalues,{l_states:current_states})\n",
    "\n",
    "#predict q-values for next states\n",
    "predicted_next_qvalues = get_output(l_qvalues,{l_states:next_states})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compiling agent's \"GetQValues\" function\n",
    "get_qvalues = theano.function([current_states], predicted_qvalues, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = predicted_qvalues[T.arange(actions.shape[0]),actions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and `update`\n",
    "Here we write a function similar to `agent.update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Computing target q-values under \n",
    "gamma = 0.99\n",
    "target_qvalues_for_actions = rewards + gamma * predicted_next_qvalues.max(axis=1)\n",
    "\n",
    "#zero-out q-values at the end\n",
    "target_qvalues_for_actions = (1-is_end)*target_qvalues_for_actions\n",
    "\n",
    "#don't compute gradient over target q-values (consider constant)\n",
    "target_qvalues_for_actions = theano.gradient.disconnected_grad(target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mean squared error loss function\n",
    "loss = ((target_qvalues_for_actions - predicted_qvalues_for_actions)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all network weights\n",
    "all_weights = get_all_params(l_qvalues,trainable=True)\n",
    "\n",
    "#network updates. Note the small learning rate (for stability)\n",
    "updates = lasagne.updates.sgd(loss,all_weights,learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training function that resembles agent.update(state,action,reward,next_state) \n",
    "#with 1 more argument meaning is_end\n",
    "train_step = theano.function([current_states,actions,rewards,next_states,is_end],\n",
    "                             updates=updates,allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 0.25 #initial epsilon\n",
    "\n",
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    \n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #get action q-values from the network\n",
    "        q_values = get_qvalues([s])[0] \n",
    "        \n",
    "        if np.random.random() < epsilon:\n",
    "            a = np.random.randint(0, n_actions) \n",
    "        else:\n",
    "            a = q_values.argmax()\n",
    "            \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #train agent one step. Note that we use one-element arrays instead of scalars \n",
    "        #because that's what function accepts.\n",
    "        train_step([s],[a],[r],[new_s],[done])\n",
    "        \n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:-116.693\tepsilon:0.00380\n",
      "mean reward:-117.950\tepsilon:0.00361\n",
      "mean reward:-114.054\tepsilon:0.00343\n",
      "mean reward:-119.482\tepsilon:0.00326\n",
      "mean reward:-113.759\tepsilon:0.00310\n",
      "mean reward:-119.879\tepsilon:0.00294\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.004\n",
    "for i in range(100):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    epsilon*=0.95\n",
    "    \n",
    "    print (\"mean reward:%.3f\\tepsilon:%.5f\"%(np.mean(rewards),epsilon))\n",
    "\n",
    "    if np.mean(rewards) > 0:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n",
    "    assert epsilon!=0, \"Please explore environment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon=0 #Don't forget to reset epsilon back to initial value if you want to go on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env=gym.make(\"LunarLander-v2\");env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-60f697e838c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"videos\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#unwrap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-60f697e838c5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"videos\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#unwrap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-d6941b52b8a3>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(t_max)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mnew_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#train agent one step. Note that we use one-element arrays instead of scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sergmiller/anaconda3/envs/py342/lib/python3.4/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sergmiller/anaconda3/envs/py342/lib/python3.4/site-packages/gym/wrappers/monitoring.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sergmiller/anaconda3/envs/py342/lib/python3.4/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sergmiller/anaconda3/envs/py342/lib/python3.4/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sergmiller/anaconda3/envs/py342/lib/python3.4/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sergmiller/anaconda3/envs/py342/lib/python3.4/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApplyLinearImpulse\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mox\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0moy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpulse_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sergmiller/anaconda3/envs/py342/lib/python3.4/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mBeginContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mcontactListener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mBeginContact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#unwrap \n",
    "env = env.env.env\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later\n",
    "\n",
    "#Warning! If you keep seeing error that reads something like\"DoubleWrapError\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['openaigym.video.3.37347.video000008.mp4', 'openaigym.video.3.37347.video000027.mp4', 'openaigym.video.3.37347.video000000.mp4', 'openaigym.video.3.37347.video000001.mp4', 'openaigym.video.3.37347.video000064.mp4']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.3.37347.video000008.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "print(video_names)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[0])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
