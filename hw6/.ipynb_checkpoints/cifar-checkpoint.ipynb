{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR Conv Net\n",
    "\n",
    "И так, в этом ноутбуке Вы сделаете превую в своей жизни сверточную сеть! На сложном датасете. Cкачайте его кстати, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir cifar10\n",
    "!curl -o cifar-10-python.tar.gz https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "!tar -xvzf cifar-10-python.tar.gz -C cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.vq import whiten\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar import load_CIFAR10\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) \n",
    "\n",
    "cifar10_dir = './cifar10/cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3, 32, 32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = whiten(X_train)\n",
    "X_test = whiten(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.911570213067062, 1.0102662581806696)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X_train), np.std(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.mean(X_train)\n",
    "S = np.std(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "# num_classes = len(classes)\n",
    "# samples_per_class = 7\n",
    "# for y, cls in enumerate(classes):\n",
    "#     idxs = np.flatnonzero(y_train == y)\n",
    "#     idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "#     for i, idx in enumerate(idxs):\n",
    "#         plt_idx = i * num_classes + y + 1\n",
    "#         plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "#         plt.imshow(X_train[idx].astype('uint8').transpose(1, 2, 0))\n",
    "#         plt.axis('off')\n",
    "#         if i == 0:\n",
    "#             plt.title(cls)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">First of all -- Checking Questions</h1> \n",
    "\n",
    "**Вопрос 1**: Чем отличаются современные сверточные сети от сетей 5 летней давности?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "**Вопрос 2**: Какие неприятности могут возникнуть во время обучения современных нейросетей?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "\n",
    "**Вопрос 3**: У вас есть очень маленький датасет из 100 картинок, классификация, но вы очень хотите использовать нейросеть, какие неприятности вас ждут и как их решить? что делать если первый вариант  решения не заработает?\n",
    "\n",
    "Сделаю аугментацию данных(например добавлю повернутые/отраженные/инвертированные картинки)\n",
    "\n",
    "**Вопрос 4**: Как сделать стайл трансфер для музыки? oO\n",
    "\n",
    "Это рассказывали на 1 доп семинаре по машинному обучению. Можно воспользоваться той же идеей, что и при стайл трансфере изображний, только накладывать спектр одного трека на другой, рассматривая спектры как изображения размера 1xt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 5110)\n"
     ]
    }
   ],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from theano import tensor as T\n",
    "from lasagne.nonlinearities import *\n",
    "from theano import tensor as T\n",
    "from lasagne.nonlinearities import softmax\n",
    "from lasagne import init\n",
    "\n",
    "from lasagne.layers import InputLayer, DropoutLayer, FlattenLayer, DenseLayer, batch_norm\n",
    "from lasagne.layers import ElemwiseMergeLayer, ElemwiseSumLayer, NonlinearityLayer\n",
    "from lasagne.layers.dnn import Conv2DDNNLayer as ConvLayer\n",
    "from lasagne.layers import Pool2DLayer as PoolLayer\n",
    "\n",
    "\n",
    "input_X = T.tensor4(\"X\")\n",
    "target_y = T.vector(\"target Y integer\",dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соберите нейронку: \n",
    "- Many times x (Conv+Pool)\n",
    "- Many small convolutions like 3x3\n",
    "- Batch Norm \n",
    "- Residual Connection\n",
    "- Data Augmentation \n",
    "- Learning rate Schedule \n",
    "- ...\n",
    "\n",
    "### Для вдохновения \n",
    "- http://torch.ch/blog/2015/07/30/cifar.html\n",
    "- https://github.com/szagoruyko/wide-residual-networks \n",
    "\n",
    "### Самое интересное\n",
    "- Для сдачи задания нужно набрать на точность тесте > **92.5**% (это займет много времени, торопитесь :) )\n",
    "- Для получения бонусных баллов > **95.0**%\n",
    "- Будет очень хорошо если вы придумаете свою архитектуру или сможете обучить что-то из вышеперечисленного :)\n",
    "- А для обучения всего этого добра вам будет куда удобнее использовать GPU на Amazon \n",
    "    - Инструкция https://github.com/persiyanov/ml-mipt/tree/master/amazon-howto \n",
    "    - Вам помогут tmux, CuDNN, ssh tunnel, nvidia-smi, ... \n",
    "    - Have fun :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# так себе ~50-70%\n",
    "def build_model_1(X):\n",
    "    input_layer = lasagne.layers.InputLayer(shape=(None,3, 32, 32), input_var=X)\n",
    "\n",
    "    # nnet = lasagne.layers.BatchNormLayer(input_layer, gamma=np.ones(3), epsilon=1e-9)\n",
    "    nnet = lasagne.layers.Conv2DLayer(input_layer, num_filters=10, filter_size=(8, 8),\n",
    "                                      pad='valid', W=init.GlorotNormal(), nonlinearity=sigmoid)\n",
    "    nnet = lasagne.layers.MaxPool2DLayer(nnet, pool_size=(2, 2))\n",
    "    nnet = lasagne.layers.Conv2DLayer(input_layer, num_filters=40, filter_size=(8, 8),\n",
    "                                      pad='valid', W=init.GlorotNormal(), nonlinearity=sigmoid)\n",
    "    nnet = lasagne.layers.MaxPool2DLayer(nnet, pool_size=(2, 2))\n",
    "    nnet = lasagne.layers.Conv2DLayer(input_layer, num_filters=100, filter_size=(8, 8),\n",
    "                                      pad='valid', W=init.GlorotNormal(), nonlinearity=sigmoid)\n",
    "    nnet = lasagne.layers.MaxPool2DLayer(nnet, pool_size=(2, 2))\n",
    "    nnet = lasagne.layers.Conv2DLayer(nnet, num_filters=10*10, filter_size=(3, 3), \n",
    "                                      pad='valid', W=init.GlorotNormal(), nonlinearity=sigmoid)\n",
    "    nnet = lasagne.layers.Conv2DLayer(nnet, num_filters=10*10, filter_size=(3, 3), \n",
    "                                      pad='valid', W=init.GlorotNormal(), nonlinearity=sigmoid)\n",
    "    nnet = lasagne.layers.Conv2DLayer(nnet, num_filters=10*10, filter_size=(3, 3), \n",
    "                                      pad='valid', W=init.GlorotNormal(), nonlinearity=sigmoid)\n",
    "    nnet = lasagne.layers.BatchNormLayer(nnet)\n",
    "    nnet = lasagne.layers.DenseLayer(nnet, num_units=100,nonlinearity=rectify)\n",
    "    nnet = lasagne.layers.DropoutLayer(nnet,p=0.42)\n",
    "    nnet = lasagne.layers.DenseLayer(nnet,num_units=64, nonlinearity=rectify)\n",
    "\n",
    "    return lasagne.layers.DenseLayer(nnet,num_units = 10, nonlinearity=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# хорошая модель ~70-75%\n",
    "def build_model_2(X):\n",
    "    input_layer = lasagne.layers.InputLayer(shape=(None,3, 32, 32), input_var=X)\n",
    "\n",
    "    nnet = lasagne.layers.Conv2DLayer(input_layer, num_filters=64, filter_size=(5, 5), nonlinearity=sigmoid, pad='valid', W=init.GlorotNormal())\n",
    "    nnet = lasagne.layers.DropoutLayer(nnet,p=0.23)\n",
    "    nnet = lasagne.layers.Conv2DLayer(nnet, num_filters=64, filter_size=(3, 3), nonlinearity=sigmoid, pad='valid', W=init.GlorotNormal())\n",
    "    nnet = lasagne.layers.MaxPool2DLayer(nnet, pool_size=(2, 2))\n",
    "    nnet = lasagne.layers.DropoutLayer(nnet,p=0.23)\n",
    "    nnet = lasagne.layers.Conv2DLayer(nnet, num_filters=64, filter_size=(3, 3), nonlinearity=sigmoid, pad='valid', W=init.GlorotNormal())\n",
    "    nnet = lasagne.layers.MaxPool2DLayer(nnet, pool_size=(2, 2))\n",
    "    nnet = lasagne.layers.DropoutLayer(nnet,p=0.23)\n",
    "    nnet = lasagne.layers.Conv2DLayer(nnet, num_filters=64, filter_size=(3, 3), nonlinearity=sigmoid, \n",
    "                                      pad='valid', W=init.GlorotNormal())\n",
    "    nnet = lasagne.layers.MaxPool2DLayer(nnet, pool_size=(2, 2))\n",
    "    nnet = lasagne.layers.DenseLayer(nnet,num_units=64, nonlinearity=sigmoid)\n",
    "\n",
    "    return lasagne.layers.DenseLayer(nnet,num_units = 10, nonlinearity=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# тоже ничего ~70%\n",
    "def build_model_3(X):\n",
    "    net = {}\n",
    "    net['input'] = InputLayer(shape=(None, 3, 32, 32), input_var=X)\n",
    "    net['conv1'] = ConvLayer(net['input'],\n",
    "                             num_filters=192,\n",
    "                             filter_size=5,\n",
    "                             pad=2,\n",
    "                             flip_filters=False)\n",
    "    net['cccp1'] = ConvLayer(\n",
    "        net['conv1'], num_filters=160, filter_size=1, flip_filters=False)\n",
    "    net['cccp2'] = ConvLayer(\n",
    "        net['cccp1'], num_filters=96, filter_size=1, flip_filters=False)\n",
    "    net['pool1'] = PoolLayer(net['cccp2'],\n",
    "                             pool_size=3,\n",
    "                             stride=2,\n",
    "                             mode='max',\n",
    "                             ignore_border=False)\n",
    "    net['drop3'] = DropoutLayer(net['pool1'], p=0.5)\n",
    "    net['conv2'] = ConvLayer(net['drop3'],\n",
    "                             num_filters=192,\n",
    "                             filter_size=5,\n",
    "                             pad=2,\n",
    "                             flip_filters=False)\n",
    "    net['cccp3'] = ConvLayer(\n",
    "        net['conv2'], num_filters=192, filter_size=1, flip_filters=False)\n",
    "    net['cccp4'] = ConvLayer(\n",
    "        net['cccp3'], num_filters=192, filter_size=1, flip_filters=False)\n",
    "    net['pool2'] = PoolLayer(net['cccp4'],\n",
    "                             pool_size=3,\n",
    "                             stride=2,\n",
    "                             mode='average_exc_pad',\n",
    "                             ignore_border=False)\n",
    "    net['drop6'] = DropoutLayer(net['pool2'], p=0.5)\n",
    "    net['conv3'] = ConvLayer(net['drop6'],\n",
    "                             num_filters=192,\n",
    "                             filter_size=3,\n",
    "                             pad=1,\n",
    "                             flip_filters=False)\n",
    "    net['cccp5'] = ConvLayer(\n",
    "        net['conv3'], num_filters=192, filter_size=1, flip_filters=False)\n",
    "    net['cccp6'] = ConvLayer(\n",
    "        net['cccp5'], num_filters=10, filter_size=1, flip_filters=False)\n",
    "    net['pool3'] = PoolLayer(net['cccp6'],\n",
    "                             pool_size=8,\n",
    "                             mode='average_exc_pad',\n",
    "                             ignore_border=False)\n",
    "#     net['output'] = FlattenLayer(net['pool3'])\n",
    "    net['output'] = DenseLayer(net['pool3'], num_units=10, nonlinearity=softmax)\n",
    "\n",
    "    return net['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# одна из лучших ~84%\n",
    "def x2Conv3x3Block(net,n,m):\n",
    "    net = ConvLayer(net, num_filters=n, filter_size=3, pad='same', nonlinearity=rectify, W=init.GlorotNormal())\n",
    "    net = lasagne.layers.BatchNormLayer(net)\n",
    "    net = ConvLayer(net, num_filters=m, filter_size=3, pad='same', nonlinearity=rectify, W=init.GlorotNormal())\n",
    "    net = PoolLayer(net, pool_size=(2,2), stride=(2,2), mode='max')\n",
    "    return net\n",
    "\n",
    "def build_model_4(X):\n",
    "    net = InputLayer(shape=(None, 3, 32, 32), input_var=X)\n",
    "    net = x2Conv3x3Block(net, 64, 128)\n",
    "    net = x2Conv3x3Block(net, 96, 192)\n",
    "    net = x2Conv3x3Block(net, 128, 256)\n",
    "    net = ConvLayer(net, num_filters=160, filter_size=3, pad='same', nonlinearity=rectify, W=init.GlorotNormal())\n",
    "    net = ConvLayer(net, num_filters=320, filter_size=3, pad='same', W=init.GlorotNormal())\n",
    "    net = PoolLayer(net, pool_size=(4,4),mode='average_exc_pad')\n",
    "    net = DenseLayer(net, num_units=64, nonlinearity=sigmoid,W=init.GlorotNormal())\n",
    "    net = DenseLayer(net, num_units=10, nonlinearity=softmax,W=init.GlorotNormal())\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# еще немного лучше ~88%\n",
    "def x2Conv3x3BlockWithRes(layer,n,m):\n",
    "    net = layer\n",
    "    if (m != layer.output_shape[1]):\n",
    "        layer = ConvLayer(layer, m, filter_size=1, pad=0, nonlinearity=None, b=None)\n",
    "    net = ConvLayer(net, num_filters=n, filter_size=3, pad='same', nonlinearity=rectify, W=init.GlorotNormal())\n",
    "    net = batch_norm(net)\n",
    "    net = ConvLayer(net, num_filters=m, filter_size=3, pad='same', nonlinearity=None, W=init.GlorotNormal())\n",
    "    net = ElemwiseSumLayer([net, layer])\n",
    "    net = NonlinearityLayer(net, nonlinearity=rectify)\n",
    "    net = PoolLayer(net, pool_size=(2,2), stride=(2,2), mode='max')\n",
    "    return net\n",
    "    \n",
    "def build_model_5(X):\n",
    "    net = InputLayer(shape=(None, 3, 32, 32), input_var=X)\n",
    "    net = x2Conv3x3BlockWithRes(net, 64, 128)\n",
    "    net = x2Conv3x3BlockWithRes(net, 96, 192)\n",
    "    net = x2Conv3x3BlockWithRes(net, 128, 256)\n",
    "    net = ConvLayer(net, num_filters=160, filter_size=3, pad='same', nonlinearity=rectify, W=init.GlorotNormal())\n",
    "    net = batch_norm(net)\n",
    "    net = ConvLayer(net, num_filters=320, filter_size=3, pad='same', W=init.GlorotNormal())\n",
    "    net = batch_norm(net)\n",
    "    net = PoolLayer(net, pool_size=(4,4),mode='max')\n",
    "    net = DenseLayer(net, num_units=4096, nonlinearity=sigmoid,W=init.GlorotNormal())\n",
    "    net = DenseLayer(net, num_units=10, nonlinearity=softmax,W=init.GlorotNormal())\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За основу взята сеть из статьи https://habrahabr.ru/post/309302/ (она не была предназначена специально для этой задачи), которая давала около 80%. Далее сеть была сильно переработана: добавлено больше batch_norm слоев, а также добавлен слой, пробрасывающий данные между слоями сверток. При обучении вероятностно отражаются и обрезаются объекты из батча. Лернинг рейт тоже меняется по-умному. В итоге получается ~ 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = build_model_5(input_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = lasagne.layers.get_output(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = lasagne.layers.get_all_params(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted, target_y).mean()\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted, target_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, beta, gamma, W, b, W, W, beta, gamma, W, b, W, W, beta, gamma, W, b, W, W, beta, gamma, W, beta, gamma, W, b, W, b]\n"
     ]
    }
   ],
   "source": [
    "params = lasagne.layers.get_all_params(net, trainable=True)\n",
    "print params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fun = theano.function([input_X,target_y],[loss, accuracy], updates=lasagne.updates.adam(loss, params), \n",
    "                            allow_input_downcast=True)\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats as sps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вот и всё, пошли её учить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _flip(X):\n",
    "    return np.array([ np.flip(x,2) if sps.bernoulli.rvs(p=0.3) else x for x in X])\n",
    "\n",
    "def _clip(X):\n",
    "    return np.array([ np.clip(x,E-2*S,E+2*S) if sps.bernoulli.rvs(p=0.3) else x for x in X])\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "        \n",
    "def custom_rate(n):\n",
    "    return 0.001 * 0.1 ** (n/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5110, 5110)\n"
     ]
    }
   ],
   "source": [
    "print(theano.sandbox.cuda.dnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.stdout = open('/dev/stdout', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Процесс обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 50 took 108.822s\n",
      "  training loss (in-iteration):\t\t0.660699\n",
      "  train accuracy:\t\t77.11 %\n",
      "  validation accuracy:\t\t77.65 %\n",
      "Epoch 2 of 50 took 83.962s\n",
      "  training loss (in-iteration):\t\t0.517738\n",
      "  train accuracy:\t\t82.27 %\n",
      "  validation accuracy:\t\t81.82 %\n",
      "Epoch 3 of 50 took 84.005s\n",
      "  training loss (in-iteration):\t\t0.434702\n",
      "  train accuracy:\t\t85.10 %\n",
      "  validation accuracy:\t\t82.84 %\n",
      "Epoch 4 of 50 took 84.019s\n",
      "  training loss (in-iteration):\t\t0.371781\n",
      "  train accuracy:\t\t87.32 %\n",
      "  validation accuracy:\t\t83.09 %\n",
      "Epoch 5 of 50 took 84.077s\n",
      "  training loss (in-iteration):\t\t0.314091\n",
      "  train accuracy:\t\t89.24 %\n",
      "  validation accuracy:\t\t84.28 %\n",
      "Epoch 6 of 50 took 84.008s\n",
      "  training loss (in-iteration):\t\t0.275054\n",
      "  train accuracy:\t\t90.69 %\n",
      "  validation accuracy:\t\t85.39 %\n",
      "Epoch 7 of 50 took 83.999s\n",
      "  training loss (in-iteration):\t\t0.238816\n",
      "  train accuracy:\t\t91.87 %\n",
      "  validation accuracy:\t\t85.80 %\n",
      "Epoch 8 of 50 took 84.015s\n",
      "  training loss (in-iteration):\t\t0.204405\n",
      "  train accuracy:\t\t93.01 %\n",
      "  validation accuracy:\t\t85.13 %\n",
      "Epoch 9 of 50 took 84.020s\n",
      "  training loss (in-iteration):\t\t0.182377\n",
      "  train accuracy:\t\t93.67 %\n",
      "  validation accuracy:\t\t85.60 %\n",
      "Epoch 10 of 50 took 83.949s\n",
      "  training loss (in-iteration):\t\t0.159011\n",
      "  train accuracy:\t\t94.59 %\n",
      "  validation accuracy:\t\t86.56 %\n",
      "Epoch 11 of 50 took 108.713s\n",
      "  training loss (in-iteration):\t\t0.073202\n",
      "  train accuracy:\t\t97.66 %\n",
      "  validation accuracy:\t\t88.18 %\n",
      "Epoch 12 of 50 took 83.987s\n",
      "  training loss (in-iteration):\t\t0.049108\n",
      "  train accuracy:\t\t98.54 %\n",
      "  validation accuracy:\t\t88.07 %\n",
      "Epoch 13 of 50 took 84.007s\n",
      "  training loss (in-iteration):\t\t0.040177\n",
      "  train accuracy:\t\t98.80 %\n",
      "  validation accuracy:\t\t88.46 %\n",
      "Epoch 14 of 50 took 83.880s\n",
      "  training loss (in-iteration):\t\t0.033696\n",
      "  train accuracy:\t\t99.06 %\n",
      "  validation accuracy:\t\t88.34 %\n",
      "Epoch 15 of 50 took 83.561s\n",
      "  training loss (in-iteration):\t\t0.026837\n",
      "  train accuracy:\t\t99.22 %\n",
      "  validation accuracy:\t\t88.66 %\n",
      "Epoch 16 of 50 took 83.322s\n",
      "  training loss (in-iteration):\t\t0.023855\n",
      "  train accuracy:\t\t99.34 %\n",
      "  validation accuracy:\t\t88.29 %\n",
      "Epoch 17 of 50 took 83.481s\n",
      "  training loss (in-iteration):\t\t0.019648\n",
      "  train accuracy:\t\t99.45 %\n",
      "  validation accuracy:\t\t88.56 %\n",
      "Epoch 18 of 50 took 83.620s\n",
      "  training loss (in-iteration):\t\t0.016975\n",
      "  train accuracy:\t\t99.55 %\n",
      "  validation accuracy:\t\t88.60 %\n",
      "Epoch 19 of 50 took 83.587s\n",
      "  training loss (in-iteration):\t\t0.013430\n",
      "  train accuracy:\t\t99.64 %\n",
      "  validation accuracy:\t\t88.41 %\n",
      "Epoch 20 of 50 took 83.938s\n",
      "  training loss (in-iteration):\t\t0.012991\n",
      "  train accuracy:\t\t99.67 %\n",
      "  validation accuracy:\t\t88.45 %\n",
      "Epoch 21 of 50 took 108.801s\n",
      "  training loss (in-iteration):\t\t0.008572\n",
      "  train accuracy:\t\t99.77 %\n",
      "  validation accuracy:\t\t88.54 %\n",
      "Epoch 22 of 50 took 84.112s\n",
      "  training loss (in-iteration):\t\t0.007467\n",
      "  train accuracy:\t\t99.80 %\n",
      "  validation accuracy:\t\t88.55 %\n",
      "Epoch 23 of 50 took 84.068s\n",
      "  training loss (in-iteration):\t\t0.006915\n",
      "  train accuracy:\t\t99.82 %\n",
      "  validation accuracy:\t\t88.52 %\n",
      "Epoch 24 of 50 took 84.058s\n",
      "  training loss (in-iteration):\t\t0.006054\n",
      "  train accuracy:\t\t99.85 %\n",
      "  validation accuracy:\t\t88.54 %\n",
      "Epoch 25 of 50 took 84.109s\n",
      "  training loss (in-iteration):\t\t0.005957\n",
      "  train accuracy:\t\t99.86 %\n",
      "  validation accuracy:\t\t88.56 %\n",
      "Epoch 26 of 50 took 84.043s\n",
      "  training loss (in-iteration):\t\t0.005764\n",
      "  train accuracy:\t\t99.87 %\n",
      "  validation accuracy:\t\t88.53 %\n",
      "Epoch 27 of 50 took 84.069s\n",
      "  training loss (in-iteration):\t\t0.004898\n",
      "  train accuracy:\t\t99.90 %\n",
      "  validation accuracy:\t\t88.57 %\n",
      "Epoch 28 of 50 took 83.958s\n",
      "  training loss (in-iteration):\t\t0.005201\n",
      "  train accuracy:\t\t99.88 %\n",
      "  validation accuracy:\t\t88.51 %\n",
      "Epoch 29 of 50 took 83.959s\n",
      "  training loss (in-iteration):\t\t0.005049\n",
      "  train accuracy:\t\t99.90 %\n",
      "  validation accuracy:\t\t88.58 %\n",
      "Epoch 30 of 50 took 83.968s\n",
      "  training loss (in-iteration):\t\t0.003974\n",
      "  train accuracy:\t\t99.94 %\n",
      "  validation accuracy:\t\t88.61 %\n",
      "Epoch 31 of 50 took 107.817s\n",
      "  training loss (in-iteration):\t\t0.004566\n",
      "  train accuracy:\t\t99.89 %\n",
      "  validation accuracy:\t\t88.61 %\n",
      "Epoch 32 of 50 took 84.036s\n",
      "  training loss (in-iteration):\t\t0.004313\n",
      "  train accuracy:\t\t99.92 %\n",
      "  validation accuracy:\t\t88.63 %\n",
      "Epoch 33 of 50 took 84.011s\n",
      "  training loss (in-iteration):\t\t0.003927\n",
      "  train accuracy:\t\t99.91 %\n",
      "  validation accuracy:\t\t88.64 %\n",
      "Epoch 34 of 50 took 83.942s\n",
      "  training loss (in-iteration):\t\t0.004091\n",
      "  train accuracy:\t\t99.92 %\n",
      "  validation accuracy:\t\t88.69 %\n",
      "Epoch 35 of 50 took 84.012s\n",
      "  training loss (in-iteration):\t\t0.004330\n",
      "  train accuracy:\t\t99.89 %\n",
      "  validation accuracy:\t\t88.64 %\n",
      "Epoch 36 of 50 took 83.962s\n",
      "  training loss (in-iteration):\t\t0.003842\n",
      "  train accuracy:\t\t99.92 %\n",
      "  validation accuracy:\t\t88.60 %\n",
      "Epoch 37 of 50 took 83.965s\n",
      "  training loss (in-iteration):\t\t0.003888\n",
      "  train accuracy:\t\t99.91 %\n",
      "  validation accuracy:\t\t88.64 %\n",
      "Epoch 38 of 50 took 83.960s\n",
      "  training loss (in-iteration):\t\t0.004000\n",
      "  train accuracy:\t\t99.92 %\n",
      "  validation accuracy:\t\t88.63 %\n",
      "Epoch 39 of 50 took 83.995s\n",
      "  training loss (in-iteration):\t\t0.004019\n",
      "  train accuracy:\t\t99.91 %\n",
      "  validation accuracy:\t\t88.61 %\n",
      "Epoch 40 of 50 took 83.929s\n",
      "  training loss (in-iteration):\t\t0.003924\n",
      "  train accuracy:\t\t99.92 %\n",
      "  validation accuracy:\t\t88.67 %\n",
      "Epoch 41 of 50 took 108.555s\n",
      "  training loss (in-iteration):\t\t0.003596\n",
      "  train accuracy:\t\t99.94 %\n",
      "  validation accuracy:\t\t88.67 %\n",
      "Epoch 42 of 50 took 84.013s\n",
      "  training loss (in-iteration):\t\t0.003822\n",
      "  train accuracy:\t\t99.93 %\n",
      "  validation accuracy:\t\t88.65 %\n",
      "Epoch 43 of 50 took 83.985s\n",
      "  training loss (in-iteration):\t\t0.004256\n",
      "  train accuracy:\t\t99.93 %\n",
      "  validation accuracy:\t\t88.65 %\n",
      "Epoch 44 of 50 took 83.975s\n",
      "  training loss (in-iteration):\t\t0.003495\n",
      "  train accuracy:\t\t99.93 %\n",
      "  validation accuracy:\t\t88.64 %\n",
      "Epoch 45 of 50 took 83.952s\n",
      "  training loss (in-iteration):\t\t0.003790\n",
      "  train accuracy:\t\t99.92 %\n",
      "  validation accuracy:\t\t88.63 %\n",
      "Epoch 46 of 50 took 83.905s\n",
      "  training loss (in-iteration):\t\t0.003385\n",
      "  train accuracy:\t\t99.94 %\n",
      "  validation accuracy:\t\t88.63 %\n",
      "Epoch 47 of 50 took 84.009s\n",
      "  training loss (in-iteration):\t\t0.003761\n",
      "  train accuracy:\t\t99.94 %\n",
      "  validation accuracy:\t\t88.65 %\n",
      "Epoch 48 of 50 took 83.971s\n",
      "  training loss (in-iteration):\t\t0.004071\n",
      "  train accuracy:\t\t99.92 %\n",
      "  validation accuracy:\t\t88.64 %\n",
      "Epoch 49 of 50 took 83.938s\n",
      "  training loss (in-iteration):\t\t0.003298\n",
      "  train accuracy:\t\t99.95 %\n",
      "  validation accuracy:\t\t88.63 %\n",
      "Epoch 50 of 50 took 83.955s\n",
      "  training loss (in-iteration):\t\t0.003767\n",
      "  train accuracy:\t\t99.93 %\n",
      "  validation accuracy:\t\t88.63 %\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 50 #количество проходов по данным\n",
    "\n",
    "batch_size = 50 #размер мини-батча\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    if epoch % 10 == 0:\n",
    "        train_fun = theano.function([input_X,target_y],[loss, accuracy], \n",
    "                                updates=lasagne.updates.adam(loss, params,learning_rate= custom_rate(epoch)), \n",
    "                            allow_input_downcast=True)\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size,True):\n",
    "        inputs, targets = batch\n",
    "        inputs = _clip(_flip(inputs))\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, y_test, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t89.01 %\n",
      "Нужно больше магии!\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 92.5:\n",
    "    print \"Achievement unlocked: колдун 80 уровня\"\n",
    "else:\n",
    "    print \"Нужно больше магии!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ясно, что можно было легко получить больше, используя специальные готовые архитектуры преднанченные именно для этих данных, но это не очень интересно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заполните форму\n",
    "\n",
    "https://goo.gl/forms/EeadABISlVmdJqgr2 "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
